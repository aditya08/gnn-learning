#!/usr/bin/env python3
"""
Generate Perlmutter SLURM scripts for multi-node strong-scaling experiments.

Example:
  python3 scripts/generate_slurm_strong_scaling.py \
    --nodes-list 1 2 4 8 \
    --allocation m1234 \
    --mail-user you@org.edu \
    --mail-type END,FAIL \
    --job-name-prefix sage-strong \
    --output-dir slurm \
    -- --max_steps 4000 --batch_size 1024 --sync_every 1
"""

import argparse
import shlex
from pathlib import Path
from typing import List


def parse_args() -> tuple[argparse.Namespace, List[str]]:
    parser = argparse.ArgumentParser(
        description="Generate one SLURM script per node count for strong scaling on Perlmutter."
    )
    parser.add_argument("--nodes-list", type=int, nargs="+", required=True, help="Node counts to generate")
    parser.add_argument("--output-dir", type=str, default="slurm", help="Directory for generated .sbatch files")
    parser.add_argument("--job-name-prefix", type=str, default="strong-scale", help="Prefix for SLURM job names")

    # SLURM metadata/options.
    parser.add_argument("--allocation", type=str, default=None, help="Allocation/account string (#SBATCH --account)")
    parser.add_argument("--qos", type=str, default=None, help="QoS (#SBATCH --qos)")
    parser.add_argument("--partition", type=str, default=None, help="Partition (#SBATCH --partition)")
    parser.add_argument("--constraint", type=str, default="gpu", help="Constraint (#SBATCH --constraint)")
    parser.add_argument("--time", type=str, default="00:30:00", help="Walltime (#SBATCH --time)")
    parser.add_argument("--mail-user", type=str, default=None, help="Email for notifications (#SBATCH --mail-user)")
    parser.add_argument(
        "--mail-type",
        type=str,
        default=None,
        help="Comma-separated types, e.g. END,FAIL (#SBATCH --mail-type)",
    )

    # Launch layout.
    parser.add_argument("--gpus-per-node", type=int, default=4, help="Processes per node for torchrun")
    parser.add_argument("--cpus-per-task", type=int, default=32, help="CPUs per launcher task")
    parser.add_argument("--rdzv-port", type=int, default=29500, help="Rendezvous port for torchrun")

    # Runtime setup.
    parser.add_argument("--project-dir", type=str, default="$SLURM_SUBMIT_DIR", help="Project directory to cd into")
    parser.add_argument("--python-bin", type=str, default="python3", help="Python executable in batch script")
    parser.add_argument("--train-entry", type=str, default="src/train.py", help="Training entry script path")
    parser.add_argument(
        "--setup-line",
        action="append",
        default=[],
        help="Extra setup line(s) to place before launch (repeatable), e.g. 'module load pytorch'",
    )
    parser.add_argument(
        "--env",
        action="append",
        default=[],
        help="Environment assignment(s) KEY=VALUE exported in script (repeatable)",
    )

    args, extra = parser.parse_known_args()
    if extra and extra[0] == "--":
        extra = extra[1:]
    return args, extra


def sbatch_line(flag: str, value: str | None) -> str:
    if value is None or value == "":
        return ""
    return f"#SBATCH --{flag}={value}"


def render_script(args: argparse.Namespace, node_count: int, train_args: List[str]) -> str:
    job_name = f"{args.job_name_prefix}-n{node_count}"
    train_args_str = " ".join(shlex.quote(a) for a in train_args)
    env_lines = "\n".join(f"export {e}" for e in args.env)
    setup_lines = "\n".join(args.setup_line)

    return f"""#!/bin/bash
# Auto-generated by scripts/generate_slurm_strong_scaling.py
# Experiment: strong scaling with {node_count} node(s)
# Generated job name: {job_name}

#SBATCH --job-name={job_name}
#SBATCH --nodes={node_count}
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node={args.gpus_per_node}
#SBATCH --cpus-per-task={args.cpus_per_task}
#SBATCH --time={args.time}
{sbatch_line("account", args.allocation)}
{sbatch_line("qos", args.qos)}
{sbatch_line("partition", args.partition)}
{sbatch_line("constraint", args.constraint)}
{sbatch_line("mail-user", args.mail_user)}
{sbatch_line("mail-type", args.mail_type)}

set -euo pipefail

cd {args.project_dir}
{env_lines}
{setup_lines}

MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
MASTER_PORT={args.rdzv_port}

echo "Running strong scaling: nodes=$SLURM_NNODES gpus_per_node={args.gpus_per_node}"
echo "MASTER_ADDR=$MASTER_ADDR MASTER_PORT=$MASTER_PORT"

srun --nodes=$SLURM_NNODES --ntasks=$SLURM_NNODES --ntasks-per-node=1 \\
  {args.python_bin} -m torch.distributed.run \\
  --nnodes=$SLURM_NNODES \\
  --nproc_per_node={args.gpus_per_node} \\
  --node_rank=$SLURM_PROCID \\
  --rdzv_backend=c10d \\
  --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \\
  {args.train_entry} {train_args_str}
"""


def main() -> int:
    args, train_args = parse_args()
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    for n in args.nodes_list:
        if n < 1:
            raise SystemExit(f"invalid node count: {n}")
        script_path = out_dir / f"{args.job_name_prefix}_n{n}.sbatch"
        script_path.write_text(render_script(args, n, train_args))
        print(f"wrote {script_path}")

    print("done")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
